#!/usr/bin/env python3
"""
Fast LLM Integration Test
Tests only the LLM components without heavy analysis
"""

import asyncio
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add backend to path
sys.path.append(str(Path(__file__).parent))

async def test_llm_fast():
    """Fast test of LLM integration only"""
    
    print("üöÄ Fast LLM Integration Test")
    print("=" * 50)
    
    try:
        # Test 1: Import LLM enhancers directly
        print("1Ô∏è‚É£ Testing LLM Enhancer Imports...")
        
        from app.review.local_llm_enhancer import LocalLLMEnhancer
        from app.review.free_api_llm_enhancer import FreeAPILLMEnhancer
        
        print("‚úÖ LLM enhancers imported successfully")
        
        # Test 2: Initialize Local LLM
        print("\n2Ô∏è‚É£ Testing Local LLM Initialization...")
        
        local_enhancer = LocalLLMEnhancer("qwen2.5-coder:7b")
        print("‚úÖ Local LLM enhancer initialized")
        
        # Test 3: Initialize Free API LLM
        print("\n3Ô∏è‚É£ Testing Free API LLM Initialization...")
        
        free_api_enhancer = FreeAPILLMEnhancer(
            huggingface_token=os.getenv("HUGGINGFACE_TOKEN"),
            replicate_token=os.getenv("REPLICATE_TOKEN"),
            openrouter_token=os.getenv("OPENROUTER_API_KEY")
        )
        print("‚úÖ Free API LLM enhancer initialized")
        
        # Test 4: Test Local LLM Enhancement
        print("\n4Ô∏è‚É£ Testing Local LLM Enhancement...")
        
        test_finding = {
            "file": "test.py",
            "line": 1,
            "severity": "medium",
            "category": "quality",
            "message": "Test finding",
            "suggestion": "Initial suggestion"
        }
        
        enhanced = await local_enhancer.enhance_finding(
            "def test_function():\n    pass", 
            test_finding
        )
        
        if enhanced.get("llm_enhanced"):
            print("‚úÖ Local LLM enhancement successful!")
            print(f"   AI Explanation: {enhanced.get('ai_explanation', '')[:80]}...")
        else:
            print("‚ö†Ô∏è Local LLM enhancement failed")
        
        # Test 5: Test Free API LLM Enhancement
        print("\n5Ô∏è‚É£ Testing Free API LLM Enhancement...")
        
        enhanced = await free_api_enhancer.enhance_finding(
            "def test_function():\n    pass", 
            test_finding
        )
        
        if enhanced.get("llm_enhanced"):
            print("‚úÖ Free API LLM enhancement successful!")
            print(f"   Provider: {enhanced.get('provider', 'unknown')}")
            print(f"   AI Explanation: {enhanced.get('ai_explanation', '')[:80]}...")
        else:
            print("‚ö†Ô∏è Free API LLM enhancement failed")
        
        # Test 6: Check Provider Status
        print("\n6Ô∏è‚É£ Checking Provider Status...")
        
        print("üåê Available Free API Providers:")
        for provider_enum, provider in free_api_enhancer.providers.items():
            current = " (CURRENT)" if provider_enum == free_api_enhancer.current_provider else ""
            print(f"   - {provider_enum.value}{current}")
        
        # Test 7: Performance Stats
        print("\n7Ô∏è‚É£ Performance Statistics...")
        
        local_stats = local_enhancer.get_performance_stats()
        print(f"üß† Local LLM: {local_stats['success_rate_percent']:.1f}% success rate")
        print(f"üåê Free API LLM: {free_api_enhancer.total_requests} requests, {free_api_enhancer.successful_requests} successful")
        
        # Final Result
        print("\n" + "=" * 50)
        print("üéØ FAST TEST RESULTS")
        print("=" * 50)
        
        if local_enhancer and free_api_enhancer:
            print("üéâ SUCCESS: Both LLM enhancers are working perfectly!")
            print("üí° Your code review agent has:")
            print("   ‚úÖ Local LLM (Qwen2.5-coder:7b)")
            print("   ‚úÖ Free API LLM (OpenRouter, Hugging Face, Replicate)")
            print("   ‚úÖ Automatic fallback between providers")
            print("   ‚úÖ Professional AI explanations")
            print("\nüöÄ Ready for production use!")
        else:
            print("‚ùå FAILURE: Some LLM enhancers are not working")
        
        # Cleanup
        local_enhancer.cleanup()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("üöÄ Starting Fast LLM Test...")
    asyncio.run(test_llm_fast())
